\documentclass{article}
\author{Andrea Casalino}
\title{Gaussian Processes}

\usepackage{graphicx,color, import}
\usepackage{amssymb, amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage[toc,page]{appendix}

\begin{document}
\maketitle

\newpage
\section{What is a Gaussian Process?}

\textbf{Gaussian Processes} \cite{GP_general_01}, \cite{GP_general_02}, a.k.a. \textbf{GP}s, are
data driven probabiistic models able to approximate generic multivariate and possibly vectorial real functions $G$ defined like that:
\begin{eqnarray}
G : \mathcal{I} \subseteq \mathbb{R}^i \rightarrow \mathcal{O} \subseteq \mathbb{R}^o
\end{eqnarray}

In essence, \textbf{GP}s are defined by their own \textbf{Training Set} and \textbf{Kernel Function}.
The \textbf{Training Set} is a collection of points pertaining to $\mathcal{I}$, for which the corresponding output, or at least thst value summed with noise, inside $\mathcal{O}$ is known.
\\
The \textbf{Kernel Function} is something that has to be chosen and typical of the kind of function to approximate. Definitions and meanings of possible \textbf{Kernel Function} are extensively detailed in Section \ref{sec:kernel_function}.
\\
The aim of \textbf{GP}s is to be able to predict the value of $G$ for a point inside $\mathcal{I}$ that is not inside of the trainin set. In particular, this is done in probabilistic terms, as the result of the prediction is not just a value, but a conditioned Gaussian distribution.
\\
Section \ref{sec:scalar} discusses the formulation of scalar \textbf{GP}, i.e. cases for which $\mathcal{O} \subseteq \mathbb{R}$. Insted, Section \ref{sec:vectorial} focuses on the more general cases. The reader will notice that the two formulations are not in contradiction and the decision to discuss them into separate Sections is only for the purpose of a better readability. 

\section{Scalar case}
\label{sec:scalar}

The training set of a scalar \textbf{GP} is a collection of points inside $\mathcal{I}$ for which the corresponding value inside $\mathcal{O}$ is known. More formally:
\begin{eqnarray}
S &=& 
\bigg \lbrace 
\begin{bmatrix} X_1 \\ \downarrow \\ y_1 \end{bmatrix}, 
\hdots,  
\begin{bmatrix} X_N \\ \downarrow \\ y_N \end{bmatrix}
\bigg \rbrace \\
\big \lbrace X_1, \hdots, X_N \big \rbrace &\subset & \mathbb{R}^i \\
\big \lbrace y_1, \hdots, y_N \big \rbrace &\subset & \mathbb{R}
\end{eqnarray}

\textbf{GP}s consider values inside the training set to be somehow correlated, as they were generated from the same underlying function.
In particular, the joint probability distribution describing such correlation is assumed to be the following 0 mean \textbf{Gaussian Distribution}:
\begin{eqnarray}
\begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix} \sim \mathcal{N} 
\bigg (
0, K \big( X_{1, \hdots, N} \big)
\bigg )  
\label{eq:y_joint_distr}
\end{eqnarray}

$K$ is a covariance matrix induced by the choice of a certain kernel function $k$ (refer also to Section \ref{sec:kernel_function}):
\begin{eqnarray}
K = \begin{bmatrix}
k(X_1, X_1, \Theta) & \hdots & k(X_1, X_N, \Theta) \\ 
\vdots & \ddots & \vdots \\ 
k(X_N, X_1, \Theta) & \hdots & k(X_N, X_N, \Theta) \\ 
\end{bmatrix}
\end{eqnarray}
where $\Theta$ is a vector of hyperparameters that are typical of the chosen kernel function and whose values can be tuned also by training (see Sections \ref{sec:train_scalar} and \ref{sec:train_vectorial}):
\begin{eqnarray}
\Theta = \begin{bmatrix} \theta_1 & \hdots & \theta_m \end{bmatrix} ^ T
\end{eqnarray}

\subsection{Predictions}
\label{sec:predictions_scalar}

The aim of a \textbf{GP} is to be able to make predictions about the output value $y = G(X)$ pertaining o an input $X$ that is outside of the training set. This is done assuming again a joint Gaussian correlation between such an additional point and all the ones in the training set: 
\begin{eqnarray}
\begin{bmatrix} y = G(X) \\ \hline y_1 \\ \vdots \\ y_N \end{bmatrix} \sim \mathcal{N} 
\bigg (
\begin{bmatrix} 0 \\ \hline 0 \end{bmatrix},
\begin{bmatrix}
k(X,X, \Theta) & \vline & K_x(X, X_{1,\hdots,N}, \Theta)^T \\ 
\hline
K_x(X, X_{1,\hdots,N}, \Theta) & \vline & K
\end{bmatrix}
\bigg )
\end{eqnarray}
where $K_x$ is a vector assembled in this way:
\begin{eqnarray}
K_x(X, X_{1,\hdots,N}, \Theta) = \begin{bmatrix}
k(X, X_1 , \Theta) & \hdots & k(X, X_N , \Theta)
\end{bmatrix}
\label{eq:additional_joint}
\end{eqnarray}

Since eq. \ref{eq:additional_joint} describes a Gaussian distibution, the conditioned distribution involving only $X$ can be obtained as follows:
\begin{eqnarray}
y(X | X_{1,\hdots,N}) 
\sim \mathcal{N} \bigg ( 
K_x K^{-1} \begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix} , 
\sigma_K(X)
\bigg )  
\label{eq:scalar_prediction}
\end{eqnarray}
where $\sigma_K(X)$ is the covariance of the conditioned distribution and can be computed as follows:
\begin{eqnarray}
\sigma_K(X) =
k(X,X) - K_x K^{-1} K_x^T
\end{eqnarray}

The distribution described by eq. \ref{eq:scalar_prediction}, actually represents the prediction made by the \textbf{GP}. Eq. \ref{eq:scalar_prediction} can also be rewritten as follows:
\begin{eqnarray}
y(X | X_{1,\hdots,N}) 
\sim \mathcal{N} \bigg (
\begin{bmatrix} y_1 & \hdots & y_N \end{bmatrix}
K^{-1} K_x^T, 
\sigma_K(X)
\bigg ) 
\end{eqnarray}

\subsection{Training}
\label{sec:train_scalar}

Training is done maximizing the likelihood $L$ of the training set w.r.t. the hyperparameters $\Theta$ of the kernel function.
Since eq. (\ref{eq:y_joint_distr}) describes a \textbf{Gaussian} distribution, the likelihood can be computed as follows:
\begin{eqnarray}
L = 
\frac{1}{\sqrt{(2 \pi)^N \left | K \right | }}
exp \bigg ( - \frac{1}{2}
\begin{bmatrix} y_1 & \hdots & y_N \end{bmatrix} 
K^{-1}
\begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix} 
\bigg ) 
\end{eqnarray}
At this point, the property described in appendix \ref{sec:trace_property} can be exploited to rewrite the above equation as follows:
\begin{eqnarray}
L &=& 
\frac{1}{\sqrt{(2 \pi)^N \left | K \right | }}
exp \bigg ( - \frac{1}{2}
Tr \bigg [ 
K^{-1}
\begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix} 
\begin{bmatrix} y_1 & \hdots & y_N \end{bmatrix} 
\bigg ]
\bigg ) \\
&=&
\frac{1}{\sqrt{(2 \pi)^N \left | K \right | }}
exp \bigg ( - \frac{1}{2}
Tr \bigg [ 
K^{-1}
M_{YY}
\bigg ]
\bigg )
\end{eqnarray}

Passing to the logarithm we obtain:
\begin{eqnarray}
\mathcal{L} = log(L) = 
-\frac{N}{2}(2 \pi) 
-\frac{1}{2} log \big (\left | K \right | \big )
-\frac{1}{2} Tr \bigg [
K^{-1} M_{YY}
\bigg ] 
\end{eqnarray}
keeping in mind that $\mathcal{L}$ is a function of the hyperparameters $\Theta$:
\begin{eqnarray}
\mathcal{L}(\Theta) = 
-\frac{N}{2}(2 \pi) 
-\frac{1}{2} log \big (\left | K(\Theta) \right | \big )
-\frac{1}{2} Tr \bigg [
K(\Theta)^{-1} M_{YY}
\bigg ] 
\end{eqnarray}

The gradient of $\mathcal{L}$ w.r.t. the generic hyperparameter $\theta_t$ is computed as follows (refer to the properties detailed at \cite{CookBook}):
\begin{eqnarray}
\frac{\partial \mathcal{L}}{\partial \theta_t} &=& 
-\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} \bigg ]
-\frac{1}{2} Tr \bigg [ \frac{\partial }{\partial \theta_t} \big (  
K^{-1} M_{YY}
\big ) \bigg ] \\
&=& -\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} \bigg ]
-\frac{1}{2} Tr \bigg [ \frac{\partial (K^{-1}) }{\partial \theta_t} M_{YY} \bigg ] \\
&=& -\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} \bigg ]
+\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} K^{-1} M_{YY} \bigg ] \\
&=&
\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} \bigg (K^{-1} M_{YY} - I_{N \times N} \bigg ) \bigg ]
 \label{eq:L_grad}
\end{eqnarray}

Choosing your favourite gradient-based approach you can tune the model, byt computing the gradient as described by the above equation.

\section{Vectorial case}
\label{sec:vectorial}

Vectorial \textbf{GP}s are defined as similarly done for the scalar case detailed in the previous Section.
The training set should now account for the multi-dimensionality of the process and is therefore defined in this way:
\begin{eqnarray}
S &=& 
\bigg \lbrace 
\begin{bmatrix} X_1 \\ \downarrow \\ Y_1 \end{bmatrix}, 
\hdots,  
\begin{bmatrix} X_N \\ \downarrow \\ Y_N \end{bmatrix}
\bigg \rbrace \\
\big \lbrace X_1, \hdots, X_N \big \rbrace &\subset & \mathbb{R}^i \\
\big \lbrace Y_1, \hdots, Y_N \big \rbrace &\subset & \mathbb{R}^o
\end{eqnarray}
The generic $Y_k$ is a vector made of $o$ components:
\begin{eqnarray}
Y_k = \begin{bmatrix} y_k^1 & \hdots & y_k^o \end{bmatrix}^T
\end{eqnarray}

\subsection{Predictions}

A vectorial \textbf{GP} is actually a composition of independent scalar \textbf{GP}s.
The prediction is done as similarly discussed in Section \ref{sec:predictions_scalar}, doing $o$ predictions at the same time. Indeed, for each component $y^{k \in \lbrace 1, \hdots ,o\rbrace}$ of the prediction  holds equation \ref{eq:scalar_prediction}. Therefore, the complete prediction can be obtained in this way:
\begin{eqnarray}
Y(X| X_{1,\hdots,N}) = \begin{bmatrix}
\mathcal{N} \bigg (
\begin{bmatrix} y^1_1 & \hdots & y^1_N \end{bmatrix}
K^{-1} K_x^T, 
\sigma_K(X)
\bigg ) \\ 
\vdots \\
\mathcal{N} \bigg (
\begin{bmatrix} y^o_1 & \hdots & y^o_N \end{bmatrix}
K^{-1} K_x^T, 
\sigma_K(X)
\bigg ) 
\end{bmatrix} 
\end{eqnarray}
the above expression can be further elaborated, leading to the distribution of an isotropic Gaussian:
\begin{eqnarray}
Y(X| X_{1,\hdots,N}) 
&\sim &
\mathcal{N} \bigg (
\begin{bmatrix} 
y^1_1 & \hdots & y^1_N \\
\vdots & \ddots & \vdots \\
y^o_1 & \hdots & y^o_N
\end{bmatrix} K^{-1} K_x^T
, \sigma_K(X) I_{o \times o}
\bigg )
\\
&\sim &
\mathcal{N} \bigg (
\begin{bmatrix} Y_1 | & \hdots & | Y_N \end{bmatrix} K^{-1} K_x^T
, \sigma_K(X) I_{o \times o}
\bigg )
\end{eqnarray}

\subsection{Training}
\label{sec:train_vectorial}

The logarithmic likelihood is the summation of the logarithmic likelihood of each process that compose the vectorial \textbf{GP}, which leads, omitting constant terms, to:
\begin{eqnarray}
\mathcal{L} &=& 
\sum_{k=1}^o \bigg (
-\frac{1}{2} log \big (\left | K \right | \big )
-\frac{1}{2} Tr \bigg [
K^{-1} 
\begin{bmatrix} y^i_1 \\ \vdots \\ y^i_N \end{bmatrix}
\begin{bmatrix} y^i_1 & \hdots & y^i_N \end{bmatrix}
\bigg ] \bigg )  \\
&=&
-\frac{o}{2} log \big (\left | K \right | \big )
-\frac{1}{2} \sum_{k=1}^o \bigg ( Tr \bigg [
K^{-1} 
\begin{bmatrix} y^i_1 \\ \vdots \\ y^i_N \end{bmatrix}
\begin{bmatrix} y^i_1 & \hdots & y^i_N \end{bmatrix}
\bigg ] 
\bigg ) \\
&=&
-\frac{o}{2} log \big (\left | K \right | \big )
-\frac{1}{2} Tr \bigg [
K^{-1} \sum_{k=1}^o \bigg (
\begin{bmatrix} y^i_1 \\ \vdots \\ y^i_N \end{bmatrix}
\begin{bmatrix} y^i_1 & \hdots & y^i_N \end{bmatrix}
\bigg )
\bigg ] \\
&=&
-\frac{o}{2} log \big (\left | K \right | \big )
-\frac{1}{2} Tr \bigg [
K^{-1} M^o_{YY}
\bigg ] 
\end{eqnarray}
with:
\begin{eqnarray}
M^o_{YY} 
&=& \sum_{k=1}^o \bigg (
\begin{bmatrix} y^i_1 \\ \vdots \\ y^i_N \end{bmatrix}
\begin{bmatrix} y^i_1 & \hdots & y^i_N \end{bmatrix}
\bigg ) \\
&=& \sum_{k=1}^o 
\begin{bmatrix}
y^i_1 y^i_1 & \hdots & y^i_1 y^i_N \\ 
\vdots & \ddots & \vdots \\ 
y^i_N y^i_1 & \hdots & y^i_N y^i_N
\end{bmatrix} \\
&=& \begin{bmatrix}
\sum_{k=1}^o y^i_1 y^i_1 & \hdots & \sum_{k=1}^o y^i_1 y^i_N \\ 
\vdots & \ddots & \vdots \\ 
\sum_{k=1}^o y^i_N y^i_1 & \hdots & \sum_{k=1}^o y^i_N y^i_N
\end{bmatrix} \\
&=& \begin{bmatrix}
<Y_1, Y_1> & \hdots  & <Y_1, Y_N> \\ 
\vdots & \ddots & \vdots \\ 
<Y_N, Y_1> & \hdots & <Y_N, Y_N>
\end{bmatrix}
\end{eqnarray}
keeping again in mind that $\mathcal{L}$ is a function of the hyperparameters $\Theta$:
\begin{eqnarray}
\mathcal{L}(\Theta) = -\frac{o}{2} log \big (\left | K \big (\Theta \big ) \right | \big )
-\frac{1}{2} Tr \bigg [
K^{-1}\big (\Theta \big ) M^o_{YY}
\bigg ] 
\end{eqnarray}
The gradient can be computed with the same steps that led to 
eq. (\ref{eq:L_grad}), leading to:
\begin{eqnarray}
\frac{1}{2} Tr \bigg [ K^{-1} \frac{\partial K}{\partial \theta_t} \bigg (K^{-1} M^o_{YY} - o I_{N \times N} \bigg ) \bigg ]
\end{eqnarray}

\section{Kernel functions}
\label{sec:kernel_function}

The kernel function describes correlation between inputs. Ideally, it should assume a low value for inputs that are "far", w.r.t. a certain metrics, from each other and high values for those inputs that are close.
\\
The kernel function $k$ should be designed in order to produce a symemtric positive definite matrix $K$, as this latter should be representative of a covariance matrix, see eq. (\ref{eq:y_joint_distr}).
\\
Clearly, the gradient of $K$ can be computed element by element:
\begin{eqnarray}
\frac{\partial K}{\partial \theta_t} = 
\begin{bmatrix}
\frac{\partial k(X_1, X_1)}{\partial \theta_t} & \hdots & \frac{\partial k(X_1, X_N)}{\partial \theta_t} \\ 
\vdots & \ddots & \vdots \\ 
\frac{\partial k(X_N, X_1)}{\partial \theta_t} & \hdots & \frac{\partial k(X_N, X_N)}{\partial \theta_t} \\ 
\end{bmatrix}
\end{eqnarray}
\\
In the following of this Section, the most popular kernel functions \footnote{Which are also the ones default supported by this package.}  will be discussed.

\subsection{Linear function}

Hyperparameters:
\begin{eqnarray}
\Theta = \begin{bmatrix} \theta_0 & \theta_1 & \mu_1 & \hdots & \mu_o \end{bmatrix}
\end{eqnarray}

Fuction evaluation:
\begin{eqnarray}
k(a,b,\Theta) 
&=&
\theta_0^2 + \theta_1^2 \big ( a-\mu \big )^T \big ( b-\mu \big ) \\
&=&
\theta_0^2 + \theta_1^2 \big ( <\mu, \mu> + <a, b> - <\mu, a + b> \big )
\end{eqnarray}

Function gradient:
\begin{eqnarray}
\frac{\partial k (a,b)}{\partial \theta_0} &=& 2 \theta_0 \\
\frac{\partial k (a,b)}{\partial \theta_1} &=& 2 \theta_1 \big ( a - \mu \big ) \big ( b - \mu \big ) \\
\begin{bmatrix} 
\frac{\partial k (a,b)}{\partial \mu_1} 
\\ 
\vdots
\\ 
\frac{\partial k (a,b)}{\partial \mu_o} 
\end{bmatrix}
&=& \theta_1^2 \big ( 2\mu - a - b \big )
\end{eqnarray}

\subsection{Squared exponential}

Hyperparameters:
\begin{eqnarray}
\Theta = \begin{bmatrix} \theta_0 & \theta_1 \end{bmatrix}
\end{eqnarray}

Fuction evaluation:
\begin{eqnarray}
k(a,b,\Theta) 
&=& 
\theta_0^2 exp \big ( -\theta_1^2 \left \| a - b \right \| ^2_2 \big ) \\
&=&
\theta_0^2 exp \big ( -\theta_1^2 (a-b)^T (a-b) \big )
\end{eqnarray}

Function gradient:
\begin{eqnarray}
\frac{\partial k (a,b)}{\partial \theta_0} &=& 2 \theta_0 exp \big ( 
-\theta_1^2 \left \| a - b \right \| ^2_2 
\big ) \\
\frac{\partial k (a,b)}{\partial \theta_1} &=& \theta_0^2 exp \big ( 
-\theta_1^2 \left \| a - b \right \| ^2_2 
\big )
\big ( -2 \theta_1(a-b)^T(a-b) \big )
\end{eqnarray}

\subsection{What to expect from the predictions}

TODO spiegare che incertezza aumenta tanto piu sonon lontano da punti in training set

\appendix

\section{Trace property}
\label{sec:trace_property}

Take an $(n,n)$ matrix $A$ and a vector $X$, the scalar quantity $x^T A x$ is equal to:
\begin{eqnarray}
x^T A x &=& Tr \bigg [ 
A xx^T
\bigg ]
\label{eq:Tr_property}
\\
&=& Tr \bigg [ 
xx^T A^T
\bigg ]
\end{eqnarray}
Clearly, in case of symmetric matrix, the following holds:
\begin{eqnarray}
x^T A x = Tr \bigg [ 
xx^T A
\bigg ]
\end{eqnarray}

We will now prove equation (\ref{eq:Tr_property}).
\\
$x^T A x$ can be also expressed as follows:
\begin{eqnarray}
x^T A x &=& x^T \begin{bmatrix} 
a_1^T \\
\vdots \\ 
a_n^T
\end{bmatrix} x \\
&=&
x^T \begin{bmatrix} 
a_1^T x \\
\vdots \\ 
a_n^T x
\end{bmatrix} = x^T \begin{bmatrix} 
<a_1, x> \\
\vdots \\ 
<a_n, x>
\end{bmatrix} \\
&=&
\sum_{i = 1}^n x_i <a_i, x>
\label{eq:sum_dots_a}
\end{eqnarray}
where $a_i$ is the $i^{th}$ row of $A$.
At the same time, the following fact is also true:
\begin{eqnarray}
Tr \bigg [ A x x^T \bigg ] &=& 
Tr \bigg [
\begin{bmatrix} a_1^T \\ \vdots \\ a_n^T \end{bmatrix}
\begin{bmatrix} xx_1 & \hdots & xx_n \end{bmatrix}
\bigg ]  \\
&=&
Tr \bigg [
\begin{bmatrix} 
a_1^T xx_1 & & \\
& \ddots & \\
& & a_n^T xx_n
\end{bmatrix}
\bigg ] \\
&=&
\sum_{i = 1}^n x_i <a_i, x>
\label{eq:sum_dots_b}
\end{eqnarray}
where we recognize that eq. (\ref{eq:sum_dots_a}) and (\ref{eq:sum_dots_b}) are identical.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
